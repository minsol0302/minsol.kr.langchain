"""RAG ì²´ì¸ ì„¤ì • ë° ê´€ë¦¬.

ì´ ëª¨ë“ˆì€ **LLMì„ ì™¸ë¶€ì—ì„œ ì£¼ì…**ë°›ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- ê¸°ë³¸ ë™ì‘: `llm` ì¸ìë¥¼ ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ OpenAI API í‚¤ë¥¼
  ê¸°ì¤€ìœ¼ë¡œ ì²´ì¸ì„ êµ¬ì„±í•˜ê±°ë‚˜, í‚¤ê°€ ì—†ì„ ë•ŒëŠ” ë”ë¯¸ ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
- ì£¼ì… ë°©ì‹: ì‚¬ìš©ìëŠ” `app.core.llm` íŒ¨í‚¤ì§€ì—ì„œ ìƒì„±í•œ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼
  `create_rag_chain(vectorstore, llm=my_llm)` í˜•íƒœë¡œ ì „ë‹¬í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from typing import Optional

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models.base import BaseLanguageModel
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import PGVector

from app.config import settings


def create_rag_chain(
    vectorstore,
    llm: Optional[BaseLanguageModel] = None,
):
    """RAG (Retrieval-Augmented Generation) ì²´ì¸ ìƒì„±.

    Args:
        vectorstore: ê²€ìƒ‰ì— ì‚¬ìš©í•  PGVector ì¸ìŠ¤í„´ìŠ¤.
        llm: ì„ íƒì  LLM ì¸ìŠ¤í„´ìŠ¤. ì£¼ì…í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Returns:
        LangChain Runnable ê°ì²´ (invoke(question: str) ì§€ì›).
    """
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = ChatPromptTemplate.from_template(
        """
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
    )

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    # 1) ì™¸ë¶€ì—ì„œ LLMì„ ì§ì ‘ ì£¼ì…í•œ ê²½ìš°
    if llm is not None:
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return rag_chain

    # 2) ì£¼ì…ëœ LLMì´ ì—†ì„ ë•Œ: ê¸°ì¡´ ë™ì‘ì„ ìœ ì§€ (OpenAI ë˜ëŠ” ë”ë¯¸ ì²´ì¸)
    if settings.openai_api_key:
        default_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | default_llm
            | StrOutputParser()
        )
        return rag_chain

    # 3) OpenAI ì„¤ì •ì´ ì—†ì„ ë•Œ: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë§Œ ë³´ì—¬ì£¼ëŠ” ë”ë¯¸ ì²´ì¸
    def dummy_rag_function(question: str) -> str:
        """OpenAI API í‚¤ê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë”ë¯¸ RAG í•¨ìˆ˜."""
        docs = retriever.invoke(question)
        context = "\n".join([f"- {doc.page_content}" for doc in docs])

        return f"""ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤:
{context}

ğŸ’¡ ë”ë¯¸ ì‘ë‹µ: ìœ„ì˜ ë¬¸ì„œë“¤ì´ '{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì…ë‹ˆë‹¤.
ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
í•˜ì§€ë§Œ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!"""

    return RunnableLambda(dummy_rag_function)

