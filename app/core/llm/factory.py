"""LLM íŒ©í† ë¦¬ í•¨ìˆ˜ - ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ìƒì„±í•©ë‹ˆë‹¤."""

from typing import Optional

from app.config import Settings
from app.core.llm.base import LLMType
from app.core.llm.providers.openai import create_openai_chat_llm
from app.core.llm.providers.korean_hf_local import create_local_korean_llm
from app.core.llm.providers.midm_local import create_midm_local_llm


def create_llm_from_config(settings: Settings) -> Optional[LLMType]:
    """ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        settings: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ê°ì²´.

    Returns:
        LLMType: ìƒì„±ëœ LLM ì¸ìŠ¤í„´ìŠ¤. ì„¤ì •ì´ ë¶ˆì™„ì „í•˜ë©´ None.

    Raises:
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM providerê°€ ì§€ì •ëœ ê²½ìš°.
        FileNotFoundError: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œê°€ ì˜ëª»ëœ ê²½ìš°.
    """
    provider = settings.llm_provider.lower()
    print(f"ğŸ” LLM Provider ì„¤ì • í™•ì¸: {provider} (ì›ë³¸: {settings.llm_provider})")

    if provider == "openai":
        if not settings.openai_api_key:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        print("ğŸ¤– OpenAI LLMì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_openai_chat_llm()

    elif provider == "korean_local":
        if not settings.local_model_dir:
            print("âš ï¸ LOCAL_MODEL_DIRì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        print(f"ğŸ  ë¡œì»¬ í•œêµ­ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {settings.local_model_dir}")
        return create_local_korean_llm(settings.local_model_dir)

    elif provider == "midm":
        print("ğŸ¤– Midm-2.0-Mini-Instruct ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # LOCAL_MODEL_DIRì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ
        model_dir = settings.local_model_dir if settings.local_model_dir else None
        try:
            llm = create_midm_local_llm(model_dir)
            print("âœ… Midm ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            return llm
        except Exception as e:
            print(f"âŒ Midm ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")
