"""ë¡œì»¬ Hugging Face í•œêµ­ì–´ LLM provider ìŠ¤ì¼ˆë ˆí†¤.

ì‹¤ì œ ë¡œì»¬ ëª¨ë¸ ë””ë ‰í„°ë¦¬(ì˜ˆ: `model.safetensors`, `config.json` ë“±)ê°€
ìˆëŠ” ê²½ë¡œë¥¼ ë°›ì•„ í•´ë‹¹ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

í˜„ì¬ëŠ” ì•„í‚¤í…ì²˜ ëª©ì ì˜ ìŠ¤ì¼ˆë ˆí†¤ë§Œ ì œê³µí•˜ë©°, êµ¬ì²´ì ì¸ ë¡œë”© ë¡œì§ì€
ì‚¬ìš©ìê°€ ì§ì ‘ êµ¬í˜„í•˜ë„ë¡ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
"""

from pathlib import Path

from app.core.llm.base import LLMType


def create_local_korean_llm(model_dir: str | Path) -> LLMType:
    """ë¡œì»¬ Hugging Face í•œêµ­ì–´ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    model_dirì—ëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
    - model.safetensors (ë˜ëŠ” pytorch_model.bin)
    - config.json
    - tokenizer.json
    - tokenizer_config.json

    Args:
        model_dir: ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í„°ë¦¬ ê²½ë¡œ.

    Returns:
        LLMType: LangChain í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤.
    """
    try:
        from langchain_community.llms import HuggingFacePipeline
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            pipeline,
            BitsAndBytesConfig
        )
        import torch
    except ImportError as e:
        raise ImportError(
            f"ë¡œì»¬ HF ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}\n"
            "pip install transformers torch langchain-community ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    model_path = Path(model_dir)
    if not model_path.exists():
        raise FileNotFoundError(f"ëª¨ë¸ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    print(f"ğŸ”§ ë¡œì»¬ í•œêµ­ì–´ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(str(model_path))

    # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì–‘ìí™” ì˜µì…˜ ê³ ë ¤)
    model_kwargs = {"torch_dtype": torch.float16 if device == "cuda" else torch.float32}

    if device == "cuda":
        # GPUì—ì„œëŠ” 4bit ì–‘ìí™” ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        model_kwargs["quantization_config"] = quantization_config
        model_kwargs["device_map"] = "auto"

    model = AutoModelForCausalLM.from_pretrained(str(model_path), **model_kwargs)

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        return_full_text=False,
        device=0 if device == "cuda" else -1,
    )

    # LangChain ë˜í¼ë¡œ ë³€í™˜
    llm = HuggingFacePipeline(pipeline=pipe)

    print("âœ… ë¡œì»¬ í•œêµ­ì–´ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    return llm
