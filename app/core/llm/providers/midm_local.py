"""Midm-2.0-Mini-Instruct ë¡œì»¬ ëª¨ë¸ provider.

K-intelligence/Midm-2.0-Mini-Instruct ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ë¡œë“œí•˜ì—¬
LangChain í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from pathlib import Path
from typing import Optional

from app.core.llm.base import LLMType


def create_midm_local_llm(model_dir: Optional[str] = None) -> LLMType:
    """Midm-2.0-Mini-Instruct ë¡œì»¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_dir: ëª¨ë¸ ë””ë ‰í„°ë¦¬ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.

    Returns:
        LLMType: LangChain í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤.

    Raises:
        ImportError: í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°.
        FileNotFoundError: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°.
    """
    try:
        from langchain_community.llms import HuggingFacePipeline
        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
        import torch
    except ImportError as e:
        raise ImportError(
            f"Midm ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}\n"
            "pip install transformers torch langchain-community ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    if model_dir is None:
        model_dir = Path(__file__).parent.parent.parent.parent / "model" / "midm"
    else:
        model_dir = Path(model_dir)

    if not model_dir.exists():
        raise FileNotFoundError(f"Midm ëª¨ë¸ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")

    print(f"ğŸ¤– Midm-2.0-Mini-Instruct ëª¨ë¸ ë¡œë”© ì¤‘: {model_dir}")

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))

        # ëª¨ë¸ ë¡œë“œ (Midm ëª¨ë¸ íŠ¹ì„±ì— ë§ê²Œ ì„¤ì •)
        print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            str(model_dir),
            torch_dtype="auto",  # ìë™ dtype ì„ íƒ
            device_map="auto",   # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘
            trust_remote_code=True,  # Midm ëª¨ë¸ í•„ìˆ˜ ì˜µì…˜
        )

        # íŒŒì´í”„ë¼ì¸ ìƒì„± (Midm ëª¨ë¸ì— ìµœì í™”ëœ ì„¤ì •)
        print("âš™ï¸ íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            return_full_text=False,
            pad_token_id=tokenizer.eos_token_id,  # íŒ¨ë”© í† í° ì„¤ì •
        )

        # LangChain ë˜í¼ë¡œ ë³€í™˜
        llm = HuggingFacePipeline(pipeline=pipe)

        print("âœ… Midm-2.0-Mini-Instruct ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return llm

    except Exception as e:
        print(f"âŒ Midm ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def create_midm_instruct_llm(model_dir: Optional[str] = None) -> LLMType:
    """Midm-2.0-Mini-Instruct ëª¨ë¸ì„ Instruct í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” create_midm_local_llmì˜ ë³„ì¹­ìœ¼ë¡œ, ëª…í™•ì„±ì„ ìœ„í•´ ì œê³µë©ë‹ˆë‹¤.
    """
    return create_midm_local_llm(model_dir)
